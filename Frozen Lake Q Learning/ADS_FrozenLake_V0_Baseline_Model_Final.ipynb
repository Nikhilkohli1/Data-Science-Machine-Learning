{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-learning\n",
    "\n",
    "Q learning is a type of value-based RL algorithm. Say we had a situation like the one below. We have a robot, that has to go till the end, avoiding the obstacles. Lets say hitting any obstacle gives you a negative reward of -10. Lets say getting to the end gives you a +50 reward. Each step gives you a negative reward of -1, because the goal is to get to the end as quick as possible. \n",
    "\n",
    "### The Q-table\n",
    "\n",
    "A Q-table is just a table that maps out the maximum expected future reward, for each action at each state. Going back to the previous example, a state would just be a block, and an action would basically be which direction you choose to move in from that block. There are 4 possible actions at each state, moving left, right, down, or up.\n",
    "\n",
    "The Q-table is basically a cheat-sheet for the agent that tells it which action is the best action to take from each state. The Q-table itself improves with each iteration of the game. \n",
    "\n",
    "\n",
    "### Q-learning Algorithm \n",
    "\n",
    "The Q function has 2 inputs, the state and the action and based on this it computes the maximum expected future reward. Here is the equation for it:\n",
    "\n",
    "Q-learning can be implemented as follows:\n",
    "\n",
    "Q(s,a)+=α⋅[r+γ⋅maxαQ(s′)−Q(s,a)]\n",
    "s: is the previous state\n",
    "\n",
    "a: is the previous action\n",
    "\n",
    "Q(): is the Q-learning algorithm\n",
    "\n",
    "s’: is the current state\n",
    "\n",
    "alpha: is the learning rate, set generally between 0 and 1. Setting the alpha value to 0 means that the Q-values are never updated, thereby nothing is learned. If we set the alpha to a high value such as 0.9, it means that the learning can occur quickly.\n",
    "\n",
    "gamma: It is the discount factor that is set between 0 and 1. This model the fact that future rewards are worth less than immediate rewards.\n",
    "\n",
    "max: is the maximum reward that is attainable in the state following the current one (the reward for taking the optimal action thereafter).\n",
    "\n",
    "The algorithm can be interpreted as:\n",
    "\n",
    "Initialize the Q-values table, Q(s, a).\n",
    "\n",
    "Observe the current state, s.\n",
    "\n",
    "Take an action, a, for that state based on the selection policy.\n",
    "\n",
    "Pick that action, and observe the reward, r, as well as the new state, s’.\n",
    "\n",
    "Now update the Q-value for the state with the help of the observed reward and the maximum reward possible for the next state.\n",
    "\n",
    "Place the state to the new state, and repeat the process until a terminal state is reached.\n",
    "\n",
    "Thus, alpha is the learning rate. If the reward or transition function is random, then alpha should change over the period, approaching zero at infinity. This has to effect approximating the expected outcome of an inner product (T(transition)*R(reward)), when one of the two, or both, has random behavior.\n",
    "\n",
    "Whereas, gamma is the value of future rewards. It can change the learning quite a bit and can be a dynamic or static value. If it is equal to one, the agent values future reward JUST AS MUCH as a current reward. This means, in ten actions, if an agent does something good this is JUST AS VALUABLE as doing this action directly. So learning doesn't work well at high gamma values.\n",
    "\n",
    "Similarly, a gamma of zero will cause the agent to only value immediate rewards, which only works with very detailed reward functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "\n",
    "- Numpy for Qtable \n",
    "- OpenAI Gym for Taxi Environment\n",
    "- Random to generate random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Install Gym and other required packages to be able to run Toy Text environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Size -  4\n",
      "State Size -  16\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print('Action Size - ',action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print('State Size - ', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 5000        # Total episodes\n",
    "total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.8                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning -------------------------------\n",
      "Number of Training Episodes - 5000\n",
      "Score over time: 0.3226\n",
      "Average Epsilon value Per Episode: 0.010407262485706588\n",
      "[[2.04733962e-03 8.74070167e-03 1.54221627e-04 1.09148711e-03]\n",
      " [9.26225724e-05 1.49412737e-03 1.09279990e-04 1.33454134e-02]\n",
      " [1.33635779e-01 1.00479008e-04 1.02779526e-04 1.01600590e-04]\n",
      " [9.93827764e-05 2.99994008e-05 4.04300930e-05 8.78764877e-05]\n",
      " [1.63879576e-02 1.18034387e-03 2.19957370e-04 1.14203648e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.18185696e-06 5.28868620e-06 6.87695340e-02 6.46897262e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.65298711e-03 2.83853794e-05 2.42035240e-03 5.54826035e-02]\n",
      " [1.22880132e-03 3.15920520e-01 1.00691144e-02 7.68806814e-03]\n",
      " [3.79656360e-03 6.31149039e-01 6.96259485e-03 3.98669071e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.67440978e-03 1.90132583e-02 5.08525400e-01 1.34158229e-02]\n",
      " [9.82111752e-02 9.36027203e-01 7.58616014e-02 5.34393616e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "rewards = []\n",
    "avg_epsilon = []\n",
    "\n",
    "print('Q-Learning -------------------------------')\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "\n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "        total_rewards += reward\n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "        if(step == max_steps-1):\n",
    "        #print('Max Step Reached for Episode - ', episode)\n",
    "        #print('Epsilon value at Max Step - ', epsilon)\n",
    "            avg_epsilon.append(epsilon)\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "    \n",
    "print(\"Number of Training Episodes - \" + str(total_episodes))\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(\"Average Epsilon value Per Episode: \" + str(sum(avg_epsilon)/len(avg_epsilon)))\n",
    "print(qtable)\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ---------------------------------\n",
      "Episode - 0,  Score -  0.0\n",
      "Episode - 1,  Score -  0.0\n",
      "Episode - 2,  Score -  0.0\n",
      "Episode - 3,  Score -  0.0\n",
      "Episode - 4,  Score -  0.0\n",
      "Episode - 5,  Score -  0.0\n",
      "Episode - 6,  Score -  0.0\n",
      "Episode - 7,  Score -  0.0\n",
      "Episode - 8,  Score -  1.0\n",
      "Episode - 9,  Score -  1.0\n",
      "Episode - 10,  Score -  0.0\n",
      "Episode - 11,  Score -  0.0\n",
      "Episode - 12,  Score -  0.0\n",
      "Episode - 13,  Score -  0.0\n",
      "Episode - 14,  Score -  1.0\n",
      "Episode - 15,  Score -  1.0\n",
      "Episode - 16,  Score -  0.0\n",
      "Episode - 17,  Score -  0.0\n",
      "Episode - 18,  Score -  0.0\n",
      "Episode - 19,  Score -  1.0\n",
      "Episode - 20,  Score -  1.0\n",
      "Episode - 21,  Score -  0.0\n",
      "Episode - 22,  Score -  1.0\n",
      "Episode - 23,  Score -  0.0\n",
      "Episode - 24,  Score -  0.0\n",
      "Episode - 25,  Score -  0.0\n",
      "Episode - 26,  Score -  1.0\n",
      "Episode - 27,  Score -  1.0\n",
      "Episode - 28,  Score -  1.0\n",
      "Episode - 29,  Score -  0.0\n",
      "Episode - 30,  Score -  0.0\n",
      "Episode - 31,  Score -  0.0\n",
      "Episode - 32,  Score -  1.0\n",
      "Episode - 33,  Score -  0.0\n",
      "Episode - 34,  Score -  0.0\n",
      "Episode - 35,  Score -  1.0\n",
      "Episode - 36,  Score -  0.0\n",
      "Episode - 37,  Score -  1.0\n",
      "Episode - 38,  Score -  0.0\n",
      "Episode - 39,  Score -  1.0\n",
      "Episode - 40,  Score -  0.0\n",
      "Episode - 41,  Score -  1.0\n",
      "Episode - 42,  Score -  1.0\n",
      "Episode - 43,  Score -  0.0\n",
      "Episode - 44,  Score -  0.0\n",
      "Episode - 45,  Score -  0.0\n",
      "Episode - 46,  Score -  1.0\n",
      "Episode - 47,  Score -  0.0\n",
      "Episode - 48,  Score -  0.0\n",
      "Episode - 49,  Score -  1.0\n",
      "Episode - 50,  Score -  0.0\n",
      "Episode - 51,  Score -  1.0\n",
      "Episode - 52,  Score -  0.0\n",
      "Episode - 53,  Score -  0.0\n",
      "Episode - 54,  Score -  0.0\n",
      "Episode - 55,  Score -  0.0\n",
      "Episode - 56,  Score -  0.0\n",
      "Episode - 57,  Score -  0.0\n",
      "Episode - 58,  Score -  0.0\n",
      "Episode - 59,  Score -  1.0\n",
      "Episode - 60,  Score -  0.0\n",
      "Episode - 61,  Score -  1.0\n",
      "Episode - 62,  Score -  1.0\n",
      "Episode - 63,  Score -  0.0\n",
      "Episode - 64,  Score -  0.0\n",
      "Episode - 65,  Score -  0.0\n",
      "Episode - 66,  Score -  0.0\n",
      "Episode - 67,  Score -  0.0\n",
      "Episode - 68,  Score -  1.0\n",
      "Episode - 69,  Score -  0.0\n",
      "Episode - 70,  Score -  1.0\n",
      "Episode - 71,  Score -  0.0\n",
      "Episode - 72,  Score -  0.0\n",
      "Episode - 73,  Score -  0.0\n",
      "Episode - 74,  Score -  1.0\n",
      "Episode - 75,  Score -  1.0\n",
      "Episode - 76,  Score -  0.0\n",
      "Episode - 77,  Score -  0.0\n",
      "Episode - 78,  Score -  1.0\n",
      "Episode - 79,  Score -  0.0\n",
      "Episode - 80,  Score -  1.0\n",
      "Episode - 81,  Score -  0.0\n",
      "Episode - 82,  Score -  0.0\n",
      "Episode - 83,  Score -  1.0\n",
      "Episode - 84,  Score -  0.0\n",
      "Episode - 85,  Score -  0.0\n",
      "Episode - 86,  Score -  1.0\n",
      "Episode - 87,  Score -  0.0\n",
      "Episode - 88,  Score -  1.0\n",
      "Episode - 89,  Score -  1.0\n",
      "Episode - 90,  Score -  1.0\n",
      "Episode - 91,  Score -  0.0\n",
      "Episode - 92,  Score -  0.0\n",
      "Episode - 93,  Score -  0.0\n",
      "Episode - 94,  Score -  1.0\n",
      "Episode - 95,  Score -  0.0\n",
      "Episode - 96,  Score -  1.0\n",
      "Episode - 97,  Score -  0.0\n",
      "Episode - 98,  Score -  1.0\n",
      "Episode - 99,  Score -  1.0\n",
      "Learning Rate value - 0.7\n",
      "Number of Episodes - 100\n",
      "Score over time: 0.37\n",
      "Average num of Steps Per Episode: 27.88\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "avg_steps = []\n",
    "\n",
    "print('Testing ---------------------------------')\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    #print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        #env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        total_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            #env.render()\n",
    "            print (\"Episode - \"+ str(episode) + \",  Score - \", total_rewards)\n",
    "            #avg_steps.append(step)\n",
    "            break\n",
    "        state = new_state\n",
    "    avg_steps.append(step)\n",
    "    rewards.append(total_rewards)\n",
    "            \n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Learning Rate value - \" + str(learning_rate))\n",
    "print(\"Number of Episodes - \" + str(total_test_episodes))\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))\n",
    "print(\"Average num of Steps Per Episode: \" + str(sum(avg_steps)/total_test_episodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Over Time for baseline Model\n",
    "\n",
    "Training - 0.3226\n",
    "\n",
    "Testing - 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions- \n",
    "\n",
    "##### 1. Establish a baseline performance. How well did your RL Q-learning do on your problem?\n",
    "\n",
    "The Q-learning did perform decently with the baseline parameters.\n",
    "I got a Training Score(Average rewards) of 0.32 and Testing 0.37. \n",
    "The average number of steps per episode is 27.88\n",
    "\n",
    "\n",
    "##### 2. What are the states, the actions and the size of the Q-table?\n",
    "\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "Actions are - Up, down, right, left\n",
    "\n",
    "Size of Q table - (state_size, action_size) (16,4)\n",
    "\n",
    "##### 3. What are the rewards? Why did you choose them?\n",
    "\n",
    "The default reward policy is - \n",
    "1 for each success\n",
    "0 for failure\n",
    "\n",
    "We will try a different policy too after tuning the hyperparameters\n",
    "\n",
    "\n",
    "##### 4. How did you choose alpha and gamma in the following equation?\n",
    "\n",
    "The alpha & gamma are default values taken from the baseline parameters provided. I will choose these later while tuning. \n",
    "\n",
    "\n",
    "##### 5. Try at least one additional value for alpha and gamma. How did it change the baseline performance?\n",
    "Done in Tuning Sheet\n",
    "\n",
    "##### 6. Try a policy other than maxQ(s', a'). How did it change the baseline performance?\n",
    "Done in Tuning Sheet\n",
    "\n",
    "##### 7. How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilonand the decay rate. How did it change the baseline performance? \n",
    "Done in Tuning Sheet\n",
    "\n",
    "##### 8. What is the value of epsilon when if you reach the max steps per episode?\n",
    "0.010407262485706588\n",
    "\n",
    "##### 9. What is the average number of steps taken per episode?\n",
    "27.88 steps per episode \n",
    "\n",
    "##### 10. Does Q-learning use value-based or policy-based iteration?\n",
    "Q-learning uses value-based iteration as it uses a policy to update the Q values. It is a off-policy. \n",
    "\n",
    "##### 11. What is meant by expected lifetime value in the Bellman equation?\n",
    "Expected Lifetime value in bellman equation is the Total Expected Reward for the Agent in its lifetime which consists of the max of Future reward discounted in time. The lifetime of the agent is one Episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
